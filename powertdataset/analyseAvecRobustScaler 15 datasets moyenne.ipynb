{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##\n",
    "##\n",
    "## PowerSystem_Dataset\n",
    "##\n",
    "##o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##\n",
    "##\n",
    "## preprocessing du dataset\n",
    "##\n",
    "##\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dans ce dataset, on calcule des scores moyens pour les 15 datasets afin qu'on puisse comparer les performances 4\n",
    "#de nos algos par rapport à ceux dans les publication\n",
    "\n",
    "#les scores de la publication sont donc ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "from statsmodels.tsa import stattools\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chaque bloc regroupe toutes les étapes nécessaires pour réaliser un score sur toutes les simulations puis faire \n",
    "# un score moyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack 0.9684563758389262\n",
      "stack 0.9598948060486522\n",
      "stack 0.963076923076923\n",
      "stack 0.9628443305573351\n",
      "stack 0.9644702842377261\n",
      "stack 0.9577181208053691\n",
      "stack 0.9618077657542966\n",
      "stack 0.9586206896551724\n",
      "stack 0.9544319600499376\n",
      "stack 0.9682824655894674\n",
      "stack 0.9587301587301588\n",
      "stack 0.9553286534779835\n",
      "stack 0.9728020240354206\n",
      "stack 0.9576547231270358\n",
      "stack 0.9519898926089703\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "scoreEtraTress = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoreStack = {'accuracy':0,'f1':0,'precision':0}\n",
    "\n",
    "for dataseti in range(1,16):\n",
    "    data_1 = pd.read_csv(\"data\"+str(dataseti)+\".csv\")\n",
    "    data_1 = data_1.replace({'Attack': 2, 'Natural': 1,'NoEvents':0})\n",
    "    is_infini = np.isinf(data_1)\n",
    "    row_has_infini = is_infini.any(axis=1)\n",
    "    rows_with_infini = data_1[row_has_infini]\n",
    "    for column in data_1:\n",
    "        is_infini = np.isinf(data_1[column]).any()\n",
    "        if is_infini == True:\n",
    "            data_1[column] = data_1[column].replace({np.inf: data_1[column][data_1[column]!=np.inf].max() *2, -np.inf: data_1[column][data_1[column]!= -np.inf].max() *2})\n",
    "    list_ndiscrt = list(data_1.columns.values)\n",
    "    list_ndiscrt.remove('marker')\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    data_1_x = data_1.drop(['marker'],axis=1)\n",
    "    data_1[list_ndiscrt] = scaler.fit_transform(data_1_x)\n",
    "    data_1_train = data_1.sample(frac=0.7)\n",
    "    data_1_test = data_1.drop(data_1_train.index)\n",
    "\n",
    "    data_1_train_x = data_1_train.drop(['marker'],axis=1)\n",
    "    data_1_train_y = data_1_train['marker']\n",
    "\n",
    "    data_1_test_x = data_1_test.drop(['marker'],axis=1)\n",
    "    data_1_test_y = data_1_test['marker']\n",
    "    \n",
    "    #********************* extra trees\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    accuracy_score(data_1_test_y,y_dct)\n",
    "    \n",
    "\n",
    "    \n",
    "    scoreEtraTress['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoreEtraTress['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoreEtraTress['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "    #********************* stack\n",
    "    estimators = [\n",
    "         ('erf', ExtraTreesClassifier()),\n",
    "         ('hist', HistGradientBoostingClassifier() ),\n",
    "         ('dtre',  tree.DecisionTreeClassifier() ),\n",
    "        ('knn',  KNeighborsClassifier() )\n",
    "\n",
    "\n",
    "    ]\n",
    "    clf = StackingClassifier(\n",
    "         estimators=estimators, final_estimator=GradientBoostingClassifier()\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(data_1_train_x, data_1_train_y).score(data_1_test_x, data_1_test_y)\n",
    "\n",
    "    \n",
    "    y_dct = clf.predict(data_1_test_x)\n",
    "    \n",
    "    print(\"stack\",accuracy_score(data_1_test_y, y_dct))\n",
    "    \n",
    "    scoreStack['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoreStack['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoreStack['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res stack average 16 datasets :\n",
      "0.9610739449062251\n",
      "0.9551635272729299\n",
      "0.958935908515271\n",
      "EtraTress average 16 datasets :\n",
      "0.9636016131528774\n",
      "0.9574773055507726\n",
      "0.9657987042594698\n"
     ]
    }
   ],
   "source": [
    "print(\"res stack average 16 datasets :\")\n",
    "print(scoreStack['accuracy']/15)\n",
    "print(scoreStack['f1']/15)\n",
    "print(scoreStack['precision']/15)\n",
    "\n",
    "    \n",
    "print(\"EtraTress average 16 datasets :\")\n",
    "print(scoreEtraTress['accuracy']/15)\n",
    "print(scoreEtraTress['f1']/15)\n",
    "print(scoreEtraTress['precision']/15)\n",
    "\n",
    "#pour stackingBoosting puis ExtraTreesClassifier\n",
    "#accuracy\n",
    "#f1\n",
    "#precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack 0.9671140939597316\n",
      "stack 0.9441157133464826\n",
      "stack 0.9612307692307692\n",
      "stack 0.9564381806534273\n",
      "stack 0.9534883720930233\n",
      "stack 0.9523489932885906\n",
      "stack 0.962444302991725\n",
      "stack 0.9473354231974922\n",
      "stack 0.9250936329588015\n",
      "stack 0.9670855774985039\n",
      "stack 0.953015873015873\n",
      "stack 0.95022335673261\n",
      "stack 0.9614168247944339\n",
      "stack 0.9602605863192183\n",
      "stack 0.9431459254579911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "scoreEtraTress = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresGradBoost = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresDECtri = {'accuracy':0,'f1':0,'precision':0}\n",
    "\n",
    "fprateET = 0\n",
    "fprateGB = 0\n",
    "fprateDT = 0\n",
    "\n",
    "\n",
    "scoreEtraTressTrainset = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresGradBoostTrainset = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresDECtriTrainset = {'accuracy':0,'f1':0,'precision':0}\n",
    "for dataseti in range(1,16):\n",
    "    data_1 = pd.read_csv(\"data\"+str(dataseti)+\".csv\")\n",
    "    data_1 = data_1.replace({'Attack': 2, 'Natural': 1,'NoEvents':0})\n",
    "    is_infini = np.isinf(data_1)\n",
    "    row_has_infini = is_infini.any(axis=1)\n",
    "    rows_with_infini = data_1[row_has_infini]\n",
    "    for column in data_1:\n",
    "        is_infini = np.isinf(data_1[column]).any()\n",
    "        if is_infini == True:\n",
    "            data_1[column] = data_1[column].replace({np.inf: data_1[column][data_1[column]!=np.inf].max() *2, -np.inf: data_1[column][data_1[column]!= -np.inf].max() *2})\n",
    "    list_ndiscrt = list(data_1.columns.values)\n",
    "    list_ndiscrt.remove('marker')\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    data_1_x = data_1.drop(['marker'],axis=1)\n",
    "    data_1[list_ndiscrt] = scaler.fit_transform(data_1_x)\n",
    "    data_1_train = data_1.sample(frac=0.7)\n",
    "    data_1_test = data_1.drop(data_1_train.index)\n",
    "\n",
    "    data_1_train_x = data_1_train.drop(['marker'],axis=1)\n",
    "    data_1_train_y = data_1_train['marker']\n",
    "\n",
    "    data_1_test_x = data_1_test.drop(['marker'],axis=1)\n",
    "    data_1_test_y = data_1_test['marker']\n",
    "    \n",
    "    #********************* extra trees\n",
    "\n",
    "\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "\n",
    "    \n",
    "    scoreEtraTress['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoreEtraTress['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoreEtraTress['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "    tp = confusion_matrix(data_1_test_y,y_dct )[0][0]\n",
    "    fp = confusion_matrix(data_1_test_y,y_dct )[0][1]\n",
    "    \n",
    "    fprateET += fp/(tn+fp)\n",
    "    \n",
    "    y_dct = model.predict(data_1_train_x)\n",
    "    \n",
    "    \n",
    "    scoreEtraTressTrainset['accuracy'] += accuracy_score(data_1_train_y, y_dct)\n",
    "    scoreEtraTressTrainset['f1'] += f1_score(data_1_train_y, y_dct, average='macro')\n",
    "    scoreEtraTressTrainset['precision'] += precision_score(data_1_train_y, y_dct, average='macro')\n",
    "    \n",
    "    #********************* HistGradientBoostingClassifier\n",
    "\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "    tp = confusion_matrix(data_1_test_y,y_dct )[0][0]\n",
    "    fp = confusion_matrix(data_1_test_y,y_dct )[0][1]\n",
    "\n",
    "    fprateGB += fp/(tn+fp)\n",
    "    print(\"stack\",accuracy_score(data_1_test_y, y_dct))\n",
    "    \n",
    "    scoresGradBoost['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoresGradBoost['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoresGradBoost['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    y_dct = model.predict(data_1_train_x)\n",
    "    scoresGradBoostTrainset['accuracy'] += accuracy_score(data_1_train_y, y_dct)\n",
    "    scoresGradBoostTrainset['f1'] += f1_score(data_1_train_y, y_dct, average='macro')\n",
    "    scoresGradBoostTrainset['precision'] += precision_score(data_1_train_y, y_dct, average='macro')\n",
    "    \n",
    "    #********************* DecisionTreeClassifier\n",
    "    \n",
    "    \n",
    "    model =  DecisionTreeClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "    tp = confusion_matrix(data_1_test_y,y_dct )[0][0]\n",
    "    fp = confusion_matrix(data_1_test_y,y_dct )[0][1]\n",
    "    fprateDT += fp/(tn+fp)\n",
    "    #print(\"stack\",accuracy_score(data_1_test_y, y_dct))\n",
    "    \n",
    "    scoresDECtri['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoresDECtri['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoresDECtri['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    y_dct = model.predict(data_1_train_x)\n",
    "    scoresDECtriTrainset['accuracy'] += accuracy_score(data_1_train_y, y_dct)\n",
    "    scoresDECtriTrainset['f1'] += f1_score(data_1_train_y, y_dct, average='macro')\n",
    "    scoresDECtriTrainset['precision'] += precision_score(data_1_train_y, y_dct, average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EtraTress average 16 datasets :\n",
      "0.9632099756529573\n",
      "0.957863427273985\n",
      "0.9692139148708365\n",
      "EtraTress train average 16 datasets :\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "HistGrandient boost average 16 datasets :\n",
      "0.9536505083692449\n",
      "0.946465157579244\n",
      "0.958047452955289\n",
      "HistGrandient Train boost average 16 datasets :\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "DT boost average 16 datasets :\n",
      "0.9094015800075762\n",
      "0.8815845568553733\n",
      "0.8779376263054796\n",
      "DT Train boost average 16 datasets :\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "fp rate ET :  0.001795755487030655\n",
      "fp rate GB :  0.005825745208295544\n",
      "fp rate DT :  0.016784025552076063\n"
     ]
    }
   ],
   "source": [
    "print(\"EtraTress average 16 datasets :\")\n",
    "print(scoreEtraTress['accuracy']/15)\n",
    "print(scoreEtraTress['f1']/15)\n",
    "print(scoreEtraTress['precision']/15)\n",
    "print(\"EtraTress train average 16 datasets :\")\n",
    "print(scoreEtraTressTrainset['accuracy']/15)\n",
    "print(scoreEtraTressTrainset['f1']/15)\n",
    "print(scoreEtraTressTrainset['precision']/15)\n",
    "\n",
    "    \n",
    "print(\"HistGrandient boost average 16 datasets :\")\n",
    "print(scoresGradBoost['accuracy']/15)\n",
    "print(scoresGradBoost['f1']/15)\n",
    "print(scoresGradBoost['precision']/15)\n",
    "print(\"HistGrandient Train boost average 16 datasets :\")\n",
    "print(scoresGradBoostTrainset['accuracy']/15)\n",
    "print(scoresGradBoostTrainset['f1']/15)\n",
    "print(scoresGradBoostTrainset['precision']/15)\n",
    "\n",
    "    \n",
    "print(\"DT boost average 16 datasets :\")\n",
    "print(scoresDECtri['accuracy']/15)\n",
    "print(scoresDECtri['f1']/15)\n",
    "print(scoresDECtri['precision']/15)\n",
    "print(\"DT Train boost average 16 datasets :\")\n",
    "print(scoresDECtriTrainset['accuracy']/15)\n",
    "print(scoresDECtriTrainset['f1']/15)\n",
    "print(scoresDECtriTrainset['precision']/15)\n",
    "\n",
    "\n",
    "#retenu pour publication hisgradient\n",
    "\n",
    "# \n",
    "\n",
    "print(\"fp rate ET : \",fprateET/15)\n",
    "print(\"fp rate GB : \",fprateGB/15)\n",
    "print(\"fp rate DT : \",fprateDT/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack 0.9570469798657718\n",
      "stack 0.9474030243261012\n",
      "stack 0.9612307692307692\n",
      "stack 0.9538757206918642\n",
      "stack 0.9450904392764858\n",
      "stack 0.9422818791946309\n",
      "stack 0.9465308720560153\n",
      "stack 0.9554858934169279\n",
      "stack 0.9431960049937578\n",
      "stack 0.9611011370436864\n",
      "stack 0.9593650793650793\n",
      "stack 0.9553286534779835\n",
      "stack 0.9645793801391525\n",
      "stack 0.9576547231270358\n",
      "stack 0.948831332912192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "scoreEtraTress = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresrf = {'accuracy':0,'f1':0,'precision':0}\n",
    "\n",
    "for dataseti in range(1,16):\n",
    "    data_1 = pd.read_csv(\"data\"+str(dataseti)+\".csv\")\n",
    "    data_1 = data_1.replace({'Attack': 2, 'Natural': 1,'NoEvents':0})\n",
    "    is_infini = np.isinf(data_1)\n",
    "    row_has_infini = is_infini.any(axis=1)\n",
    "    rows_with_infini = data_1[row_has_infini]\n",
    "    for column in data_1:\n",
    "        is_infini = np.isinf(data_1[column]).any()\n",
    "        if is_infini == True:\n",
    "            data_1[column] = data_1[column].replace({np.inf: data_1[column][data_1[column]!=np.inf].max() *2, -np.inf: data_1[column][data_1[column]!= -np.inf].max() *2})\n",
    "    list_ndiscrt = list(data_1.columns.values)\n",
    "    list_ndiscrt.remove('marker')\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    data_1_x = data_1.drop(['marker'],axis=1)\n",
    "    data_1[list_ndiscrt] = scaler.fit_transform(data_1_x)\n",
    "    data_1_train = data_1.sample(frac=0.7)\n",
    "    data_1_test = data_1.drop(data_1_train.index)\n",
    "\n",
    "    data_1_train_x = data_1_train.drop(['marker'],axis=1)\n",
    "    data_1_train_y = data_1_train['marker']\n",
    "\n",
    "    data_1_test_x = data_1_test.drop(['marker'],axis=1)\n",
    "    data_1_test_y = data_1_test['marker']\n",
    "    \n",
    "    #********************* RandomForestClassifier\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "    model = ensemble.RandomForestClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "\n",
    "    \n",
    "    scoreEtraTress['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoreEtraTress['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoreEtraTress['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "    #********************* HistGradientBoostingClassifier\n",
    "\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "    \n",
    "    print(\"stack\",accuracy_score(data_1_test_y, y_dct))\n",
    "    \n",
    "    scoresrf['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoresrf['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoresrf['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forst average 16 datasets :\n",
      "0.9532667926078304\n",
      "0.9470222287005373\n",
      "0.9595013102958169\n",
      "EtraTress average 16 datasets :\n",
      "0.9530234273151729\n",
      "0.946931978672535\n",
      "0.9645854749578281\n"
     ]
    }
   ],
   "source": [
    "print(\"HistGradientBoostingClassifier average 16 datasets :\")\n",
    "print(scoresrf['accuracy']/15)\n",
    "print(scoresrf['f1']/15)\n",
    "print(scoresrf['precision']/15)\n",
    "\n",
    "    \n",
    "print(\"random forst average 16 datasets :\")\n",
    "print(scoreEtraTress['accuracy']/15)\n",
    "print(scoreEtraTress['f1']/15)\n",
    "print(scoreEtraTress['precision']/15)\n",
    "\n",
    "#pour HistGradientBoostingClassifier  puis  RandomForestClassifier !! pas l'inverse\n",
    "#accuracy\n",
    "#f1\n",
    "#precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#il s'agit de HistGradientBoostingClassifier, pas extraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "scoreEtraTress = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresNB = {'accuracy':0,'f1':0,'precision':0}\n",
    "\n",
    "for dataseti in range(1,16):\n",
    "    data_1 = pd.read_csv(\"data\"+str(dataseti)+\".csv\")\n",
    "    data_1 = data_1.replace({'Attack': 2, 'Natural': 1,'NoEvents':0})\n",
    "    is_infini = np.isinf(data_1)\n",
    "    row_has_infini = is_infini.any(axis=1)\n",
    "    rows_with_infini = data_1[row_has_infini]\n",
    "    for column in data_1:\n",
    "        is_infini = np.isinf(data_1[column]).any()\n",
    "        if is_infini == True:\n",
    "            data_1[column] = data_1[column].replace({np.inf: 0, -np.inf: 0})\n",
    "    list_ndiscrt = list(data_1.columns.values)\n",
    "    list_ndiscrt.remove('marker')\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    data_1_x = data_1.drop(['marker'],axis=1)\n",
    "    data_1[list_ndiscrt] = scaler.fit_transform(data_1_x)\n",
    "    data_1_train = data_1.sample(frac=0.7)\n",
    "    data_1_test = data_1.drop(data_1_train.index)\n",
    "\n",
    "    data_1_train_x = data_1_train.drop(['marker'],axis=1)\n",
    "    data_1_train_y = data_1_train['marker']\n",
    "\n",
    "    data_1_test_x = data_1_test.drop(['marker'],axis=1)\n",
    "    data_1_test_y = data_1_test['marker']\n",
    "    \n",
    "    #********************* RandomForestClassifier\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "    model = ensemble.RandomForestClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "\n",
    "    \n",
    "    scoreEtraTress['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoreEtraTress['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoreEtraTress['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "    #********************* DecisionTreeClassifier\n",
    "\n",
    "    model =  DecisionTreeClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "    \n",
    "    #print(\"stack\",accuracy_score(data_1_test_y, y_dct))\n",
    "    \n",
    "    scoresNB['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoresNB['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoresNB['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree average 16 datasets :\n",
      "0.9074860965355854\n",
      "0.8861083716804269\n",
      "0.8809073955110357\n",
      "EtraTress average 16 datasets :\n",
      "0.9551758127651889\n",
      "0.9485301929855662\n",
      "0.9657838963706282\n"
     ]
    }
   ],
   "source": [
    "print(\"decision tree average 16 datasets :\")\n",
    "print(scoresNB['accuracy']/15)\n",
    "print(scoresNB['f1']/15)\n",
    "print(scoresNB['precision']/15)\n",
    "\n",
    "    \n",
    "print(\"RandomForestClassifier average 16 datasets :\") #RandomForestClassifier !!\n",
    "print(scoreEtraTress['accuracy']/15)\n",
    "print(scoreEtraTress['f1']/15)\n",
    "print(scoreEtraTress['precision']/15)\n",
    "\n",
    "#pour decision tree  puis  RandomForestClassifier \n",
    "#accuracy\n",
    "#f1\n",
    "#precision\n",
    "\n",
    "\n",
    "#retenu pour decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "confMatrixExtrtress =[]\n",
    "confMatrixHist =[]\n",
    "\n",
    "scoreEtraTress = {'accuracy':0,'f1':0,'precision':0}\n",
    "scoresNB = {'accuracy':0,'f1':0,'precision':0}\n",
    "\n",
    "for dataseti in range(1,16):\n",
    "    data_1 = pd.read_csv(\"data\"+str(dataseti)+\".csv\")\n",
    "    data_1 = data_1.replace({'Attack': 2, 'Natural': 1,'NoEvents':0})\n",
    "    is_infini = np.isinf(data_1)\n",
    "    row_has_infini = is_infini.any(axis=1)\n",
    "    rows_with_infini = data_1[row_has_infini]\n",
    "    for column in data_1:\n",
    "        is_infini = np.isinf(data_1[column]).any()\n",
    "        if is_infini == True:\n",
    "            data_1[column] = data_1[column].replace({np.inf: 0, -np.inf: 0})\n",
    "    list_ndiscrt = list(data_1.columns.values)\n",
    "    list_ndiscrt.remove('marker')\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    data_1_x = data_1.drop(['marker'],axis=1)\n",
    "    data_1[list_ndiscrt] = scaler.fit_transform(data_1_x)\n",
    "    data_1_train = data_1.sample(frac=0.7)\n",
    "    data_1_test = data_1.drop(data_1_train.index)\n",
    "\n",
    "    data_1_train_x = data_1_train.drop(['marker'],axis=1)\n",
    "    data_1_train_y = data_1_train['marker']\n",
    "\n",
    "    data_1_test_x = data_1_test.drop(['marker'],axis=1)\n",
    "    data_1_test_y = data_1_test['marker']\n",
    "    \n",
    "    #********************* extra trees\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "    model = ExtraTreesClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "\n",
    "    \n",
    "    scoreEtraTress['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoreEtraTress['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoreEtraTress['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "    \n",
    "    confMatrixExtrtress.append(confusion_matrix(data_1_test_y, y_dct))\n",
    "    #********************* HistGradientBoostingClassifier\n",
    "\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    model.fit(data_1_train_x,data_1_train_y)\n",
    "    y_dct = model.predict(data_1_test_x)\n",
    "    \n",
    "    \n",
    "    #print(\"stack\",accuracy_score(data_1_test_y, y_dct))\n",
    "    \n",
    "    scoresNB['accuracy'] += accuracy_score(data_1_test_y, y_dct)\n",
    "    scoresNB['f1'] += f1_score(data_1_test_y, y_dct, average='macro')\n",
    "    scoresNB['precision'] += precision_score(data_1_test_y, y_dct, average='macro')\n",
    "    \n",
    "    confMatrixHist.append(confusion_matrix(data_1_test_y, y_dct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree average 16 datasets :\n",
      "0.954096557748649\n",
      "0.945695565512582\n",
      "0.9578073997800923\n",
      "EtraTress average 16 datasets :\n",
      "0.963875160400772\n",
      "0.9601217724563739\n",
      "0.9682914351237409\n"
     ]
    }
   ],
   "source": [
    "print(\"HistGradientBoostingClassifier average 16 datasets :\") #HistGradientBoostingClassifier!!\n",
    "print(scoresNB['accuracy']/15)\n",
    "print(scoresNB['f1']/15)\n",
    "print(scoresNB['precision']/15)\n",
    "\n",
    "    \n",
    "print(\"EtraTress average 16 datasets :\")\n",
    "print(scoreEtraTress['accuracy']/15)\n",
    "print(scoreEtraTress['f1']/15)\n",
    "print(scoreEtraTress['precision']/15)\n",
    "\n",
    "\n",
    "#pour HistGradientBoostingClassifier  puis  ExtraTreesClassifier !! \n",
    "#accuracy\n",
    "#f1\n",
    "#precision\n",
    "\n",
    "\n",
    "#retenu pour publication pour etra trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03636363636363636\n",
      "0.02912621359223301\n",
      "0.009615384615384616\n",
      "0.0\n",
      "0.07777777777777778\n",
      "0.029411764705882353\n",
      "0.014285714285714285\n",
      "0.009615384615384616\n",
      "0.007042253521126761\n",
      "0.011494252873563218\n",
      "0.0\n",
      "0.024793388429752067\n",
      "0.125\n",
      "0.0\n",
      "0.006578947368421052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.025406981209925072"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on cherche le taux de faux positifs pour extra trees\n",
    "\n",
    "fpt = 0.0\n",
    "for i in range(len(confMatrixExtrtress)):\n",
    "    fpttemp = (confMatrixExtrtress[i][0][1]+confMatrixExtrtress[i][0][2])/(confMatrixExtrtress[i][0][1]+confMatrixExtrtress[i][0][2]+confMatrixExtrtress[i][0][0])\n",
    "    print(fpttemp)\n",
    "    fpt+=fpttemp\n",
    "fpt/len(confMatrixHist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.5% pour confMatrixExtrtress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
