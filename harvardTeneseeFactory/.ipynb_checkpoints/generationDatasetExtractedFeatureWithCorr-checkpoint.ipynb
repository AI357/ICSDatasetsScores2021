{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "from statsmodels.tsa import stattools\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "ALLtrain = pd.read_csv(\"ALLtrain\")\n",
    "#ALLtest = pd.read_csv(\"ALLtest\")\n",
    "\n",
    "ALLtrain = ALLtrain.drop(['Unnamed: 0'],axis=1)\n",
    "#ALLtest = ALLtest.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ce notebook permet d'extraire des doonées statistiques sur les courbes des sondes\n",
    "# on espère alors pouvoir d'écrire un comportement anormal grace aux stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour generer ce dataset, voir 'GenerationDataSetCorrelation.ipynb'\n",
    "\n",
    "# ce dataset nous indique les noms des sondes dont la correlation entre elles est très forte lors d'une attaque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresonderepartition = pd.read_csv('featuresonderepartition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple\n",
    "\n",
    "# pour la sonde xmeas_1, voici la liste des sondes avec lesquelles la correlation avec xmeas_1 est forte lors d'une attaque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmeas_32\n",
      "xmeas_2\n",
      "xmeas_21\n",
      "xmeas_30\n",
      "xmeas_36\n",
      "xmeas_24\n",
      "xmeas_4\n",
      "xmv_4\n",
      "xmeas_16\n",
      "xmeas_28\n",
      "xmeas_23\n",
      "xmeas_29\n",
      "xmeas_27\n"
     ]
    }
   ],
   "source": [
    "a = featuresonderepartition[featuresonderepartition['Unnamed: 0']=='xmeas_1'].drop(['Unnamed: 0'],axis=1).values.reshape(-1,)\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "starting\n",
      "simu :  1\n",
      "Index(['xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6',\n",
      "       'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12',\n",
      "       'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18',\n",
      "       'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24',\n",
      "       'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30',\n",
      "       'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36',\n",
      "       'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1',\n",
      "       'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9',\n",
      "       'xmv_10', 'xmv_11'],\n",
      "      dtype='object')\n",
      "dd\n",
      "ckdkkejk 0 165\n",
      "ckdkkejk 165 165\n",
      "ici\n",
      "['xmeas_32' 'xmeas_2' 'xmeas_21' 'xmeas_30' 'xmeas_36' 'xmeas_24'\n",
      " 'xmeas_4' 'xmv_4' 'xmeas_16' 'xmeas_28' 'xmeas_23' 'xmeas_29' 'xmeas_27']\n",
      "[0.1389969328911235, 0.01993565048459652, 0.11930389057251146, 0.06583529882229434, 0.10417348876043203, 0.08158509552864465, 0.08881859007826028, 0.06023325138499881, 0.059853926566825554, 0.030326055827393228, 0.14345134376871832, 0.017552283089886385, 0.051560347356902826]\n"
     ]
    }
   ],
   "source": [
    "print(\"init\")\n",
    "Ctraining = pd.DataFrame(columns = ['faultNumber','simulationRun','sensor','subSample','min','max','std','median','autocorr','q25','q75','q90','maxtstdfenetre','mintstdfenetre','coefRegression','gradiantfenetre','autocorrSonde0','autocorrSonde1','autocorrSonde2','autocorrSonde3','autocorrSonde4','autocorrSonde5','autocorrSonde6','autocorrSonde7','autocorrSonde8','autocorrSonde9','autocorrSonde10','autocorrSonde11','autocorrSonde12'])\n",
    "arr2 = []\n",
    "indiceMin = 165\n",
    "ecart = 165\n",
    "x = np.arange(ecart).reshape(-1,1)\n",
    "model = linear_model.LinearRegression()\n",
    "SousfenetreEcart = int(ecart/5)\n",
    "columns = ALLtrain.drop(['faultNumber','simulationRun','sample'],axis=1).columns\n",
    "print(\"starting\")\n",
    "for case in range(0,1):\n",
    "    for simulation in range(1,500):\n",
    "        if simulation > 99 and case != 0:\n",
    "            break\n",
    "        print('simu : ',simulation)\n",
    "        courbeT = ALLtrain[(ALLtrain.faultNumber == case) & (ALLtrain.simulationRun==simulation )]\n",
    "        print(columns)\n",
    "        for variableNom in columns:\n",
    "            print(\"dd\")\n",
    "            for id in range(0,501,ecart):\n",
    "                print(\"ckdkkejk\",id,ecart)\n",
    "                if id % ecart == 0 and id!=0:  \n",
    "                    print(\"ici\")\n",
    "                    serieV = courbeT[variableNom][id-ecart:id]\n",
    "                    resultsRegression = model.fit(x,serieV)\n",
    "                    mean = serieV.mean()\n",
    "                    maxtstdfenetreTempo = 0\n",
    "                    maxtstdfenetre = 0\n",
    "                    mintstdfenetre = 9999\n",
    "                    meanGradiant = 0\n",
    "                    for sousCourbeid in range(0,ecart,SousfenetreEcart):\n",
    "                        \n",
    "                        maxtstdfenetreTempo = serieV[sousCourbeid:sousCourbeid+SousfenetreEcart].std() \n",
    "                        \n",
    "                        if maxtstdfenetre<maxtstdfenetreTempo:\n",
    "                            maxtstdfenetre=maxtstdfenetreTempo\n",
    "                            \n",
    "                            tabgrad = np.gradient(serieV[sousCourbeid:sousCourbeid+SousfenetreEcart:3])\n",
    "                            meanGradiant = np.sum(tabgrad)/len(tabgrad)\n",
    "                           \n",
    "                        if maxtstdfenetreTempo < mintstdfenetre:\n",
    "                            mintstdfenetre = maxtstdfenetreTempo\n",
    "                    \n",
    "                    dicCorr = []\n",
    "                    sondesCorrList = featuresonderepartition[featuresonderepartition['Unnamed: 0']==variableNom].drop(['Unnamed: 0'],axis=1).values.reshape(-1,)\n",
    "                    \n",
    "                    for sondeNameCorr in sondesCorrList:\n",
    "                       # print(serieV.shape)\n",
    "                       # print(courbeT[sondeNameCorr][id-ecart:id])\n",
    "                        corrtemp = serieV.corr(courbeT[sondeNameCorr][id-ecart:id])\n",
    "                        if np.isnan(corrtemp):\n",
    "                            corrtemp = 1\n",
    "                        dicCorr.append(abs(corrtemp) )\n",
    "                    \n",
    "                    max = serieV.max()\n",
    "                    min = serieV.min()\n",
    "                    std = serieV.std()\n",
    "                    q25 = serieV.quantile(0.25)\n",
    "                    q50 = serieV.quantile(0.5)\n",
    "                    q75 = serieV.quantile(0.75)\n",
    "                    q90 = serieV.quantile(0.90)\n",
    "                    autocorr = serieV.autocorr()\n",
    "                    entropyRes = scipy.stats.entropy(serieV)\n",
    "                    if np.isnan(autocorr):\n",
    "                        autocorr = 0\n",
    "                    if np.isnan(entropyRes):\n",
    "                        entropyRes = 0\n",
    "                    \n",
    "                    \n",
    "                    Ctraining = Ctraining.append({'faultNumber' : case,'simulationRun' :simulation,'sensor':variableNom,'subSample':id,'min':min,'max':max,'std':std,'median':q50,'q25':q25,'q75':q75,'q90':q90,'autocorr':autocorr,'maxtstdfenetre':maxtstdfenetre,'mintstdfenetre':mintstdfenetre,'coefRegression':resultsRegression.coef_[0],'entropy':entropyRes,'gradiantfenetre':meanGradiant,'autocorrSonde0':dicCorr[0],'autocorrSonde1':dicCorr[1],'autocorrSonde2':dicCorr[2],'autocorrSonde3':dicCorr[3],'autocorrSonde4':dicCorr[4],'autocorrSonde5':dicCorr[5],'autocorrSonde6':dicCorr[6],'autocorrSonde7':dicCorr[7],'autocorrSonde8':dicCorr[8],'autocorrSonde9':dicCorr[9],'autocorrSonde10':dicCorr[10],'autocorrSonde11':dicCorr[11],'autocorrSonde12':dicCorr[12]},ignore_index=True )\n",
    "\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ctraining.to_csv('ALLDFStatsExtractedWithCorr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
